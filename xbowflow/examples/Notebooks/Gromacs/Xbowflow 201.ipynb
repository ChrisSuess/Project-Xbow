{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook illustrates how to create workflows using *Xflowlib*, and then run them efficiently using *Xflowlib*'s distributed computing capabilities.\n",
    "\n",
    "It's assumed you have done the \"Xbowflow 101\" notebook or similar, and undertand how you create *kernels* to run command-line appplications.\n",
    "\n",
    "You need to have *Gromacs* installed on the computer you are running this notebook on, and also *Xbowflow*.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "The aim of this notebook is to build on the 'Xbowflow 101\" notebook, and create a workflow to:\n",
    "\n",
    "1. Run a short MD job\n",
    "2. Energy minimise each of the structures in the resulting trajectory\n",
    "\n",
    "---\n",
    "\n",
    "Begin by creating the required kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbowflow import xflowlib\n",
    "# Build kernels for mdrun and grompp:\n",
    "md = xflowlib.SubprocessKernel('gmx mdrun -s x.tpr -c x.gro -o x.trr -g x.log')\n",
    "md.set_inputs(['x.tpr'])\n",
    "md.set_outputs(['x.trr'])\n",
    "\n",
    "em = xflowlib.SubprocessKernel('gmx mdrun -s x.tpr -c x.gro')\n",
    "em.set_inputs(['x.tpr'])\n",
    "em.set_outputs(['x.gro'])\n",
    "\n",
    "grompp = xflowlib.SubprocessKernel('gmx grompp -f x.mdp -c x.gro -p x.top -o x.tpr -maxwarn 1')\n",
    "grompp.set_inputs(['x.mdp', 'x.gro', 'x.top'])\n",
    "grompp.set_outputs(['x.tpr'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Running the workflow without distributed computing\n",
    "\n",
    "The workflow is fairly simple: \n",
    "1. Run grompp to prepare the starting structure for MD.\n",
    "2. Run the MD.\n",
    "3. For each structure in the trajectory:\n",
    "\n",
    "    a. Run grompp to prepare it for energy minimisation.\n",
    "    \n",
    "    b. Run the energy minimisation.\n",
    "    \n",
    "    c. Save the final coordinates to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_crds = xflowlib.load('bpti.gro')\n",
    "topfile = xflowlib.load('bpti.top')\n",
    "md_mdp = xflowlib.load('nvt.mdp')\n",
    "mdtpr = grompp.run(md_mdp, start_crds, topfile)\n",
    "trajectory = md.run(mdtpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as mdt\n",
    "traj = mdt.load(trajectory.as_file(), top=start_crds.as_file())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "em_mdp = xflowlib.load('em.mdp')\n",
    "for i, snapshot in enumerate(traj):\n",
    "    print('Energy minimising snapshot {}'.format(i))\n",
    "    emtpr = grompp.run(em_mdp, snapshot, topfile)\n",
    "    mincrds = em.run(emtpr)\n",
    "    mincrds.save('bpti_min_{}.gro'.format(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Running the workflow with distributed computing\n",
    "\n",
    "In distributed computing, tasks are farmed out to \"workers\". Where the program logic permits, tasks that can be run in parallel are sent to different workers. Clearly that applies to the energy minimisation steps here - they are independent of each other, and if enough workers were available, each task could be run at the same time.\n",
    "\n",
    "Xbowflow comes with a distributed computing capability built on *dask*. If you are running this notebook on your own desktop machine or equivalent, it will create a \"pool\" of workers on it to run jobs in parallel. Depending on the capabilities of your machine, you may or may not see much performance improvement compared to running the jobs without distributed computing, but if you are running this notebook on an *Xbow* cluster, then each worker is a separate compute node and you should see a significant speed-up.\n",
    "\n",
    "----\n",
    "\n",
    "Begin by creating a *client*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xbowflow.clients import XflowClient\n",
    "client = XflowClient(local=True)\n",
    "# If you are running this notebook on an xbow cluster, replace the line above with the one below:\n",
    "# client = Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have a look at the configuration of the cluster of workers that this client will interact with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client.cluster())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are going to distribute the energy minimisations of the snapshots from the trajectory file across the workers in your cluster. For performance reasons, you begin by uploading the snaphots to the cluster, using `client.upload()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshots = [client.upload(t) for t in traj]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will run the grompp and mdrun jobs in parallel across the available workers, using the `client.map()` method. This takes the name of the kernel as the first argument, and *lists* of kernel arguments after that. Each task takes one item from each of the argument lists, and evaluates the kernel using those. It then returns a *list* of kernel outputs.\n",
    "\n",
    "Thus, if a kernel had the form:\n",
    "\n",
    "    result = myfunc.run(inputa, inputb)\n",
    "\n",
    "Then this would become:\n",
    "\n",
    "    [result1, result2] = client.map(myfunc, [inputa_1, inputa_2], [inputb_1, inputb_2])\n",
    "\n",
    "However as a short-cut, if one of the arguments (e.g. inputb) is always the same, you can instead write:\n",
    "\n",
    "    [result1, result2] = client.map(myfunc, [inputa_1, inputa_2], inputb)\n",
    "    \n",
    "And `input_b` will be expanded to `[input_b, input_b]` automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the grompp jobs, then the energy minimisations:\n",
    "em_tprs = client.map(grompp, em_mdp, snapshots, topfile) # Note only snapshots is a list, other arguments get expanded automatically\n",
    "mincrds = client.map(em, em_tprs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may have been surprised that when you executed the cell above, it appeared to complete almost instantaneously - did the jobs really run that fast? \n",
    "\n",
    "No - the `client.map()` method runs the jobs asynchronously - they have been submitted to the workers, but probably have not finished yet. The variables `em_tprs` and `mincrds` are not actually the (lists of) new files - they are `futures` from which, as some time in the future, the real files can be obtained by calling their `result()` method.\n",
    "\n",
    "In the cell below you wait for the jobs to complete, and then write out the minimized coordinate files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, mincrd in enumerate(mincrds):\n",
    "    print('saving minimised snapshot {}'.format(i))\n",
    "    mincrd.result().save('bpti_min_{}.gro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
